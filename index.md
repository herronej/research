# Emily J. Herron

Postdoctoral Research Associate at Oak Ridge National Laboratory  
Analytics and AI Methods at Scale Group  
emily.j.herron@gmail.com | [CV](https://herronej.github.io/research/CV_2024.pdf) | [Google Scholar](https://scholar.google.com/citations?user=eTzUe54AAAAJ&hl=en) | [LinkedIn](https://www.linkedin.com/in/emily-herron-ph-d-623b43aa/) | [Github](https://github.com/herronej)

## About

Emily Herron is a postdoctoral research associate in the Analytics & AI Methods at Scale Group in the National Center for Computational Science at Oak Ridge National Laboratory. Her current research focuses on machine learning, particularly in neural architecture search, large language models, and AI trustworthiness. She completed her Ph.D. in Data Science & Engineering from the University of Tennessee, where she worked on generalized differentiable neural architecture search with scaling and stability improvements.

## Current Work

- Developing pipelines for assessing LLM trustworthiness in scientific applications, focusing on truthfulness, adversarial robustness, and ethics
- Incorporating evolutionary neural architecture search into mixture of experts (MoE) transformer architectures at scale
- Researching LLM-based hypothesis generation in fields of natural language processing and materials science

## Publications

**ChatHPC: Empowering HPC Users with Large Language Models**  
J. Yin et al.  
*Journal of Supercomputing, vol. 81, 2025*  
DOI: [https://doi.org/10.1007/s11227-024-06637-1](https://doi.org/10.1007/s11227-024-06637-1)

**Exploring Scientific Hypothesis Generation with Mamba**  
M. Chai, E. Herron, E. Cervantes, and T. Ghosal  
*Proceedings of the 1st Workshop on NLP for Science (NLP4Science), ACL, 2024*  
Pages 197-207

**SciTrust: Evaluating the Trustworthiness of Large Language Models for Science**  
E. Herron, J. Yin, and F. Wang  
*AI4S: 5th Workshop on Artificial Intelligence and Machine Learning for Scientific Applications, 2024*

**ICDARTS: Improving the Stability and Performance of Cyclic DARTS**  
E. Herron, D. Rose, and S. Young  
*arXiv, September 2023*  
[https://arxiv.org/abs/2309.00664](https://arxiv.org/abs/2309.00664)

**ICDARTS: Improving the Stability of Cyclic DARTS**  
E. J. Herron, S. R. Young, and D. Rose  
*2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA), 2022*

**The Sensitivity of Word Embeddings-Based Author Detection Models to Semantic-Preserving Adversarial Perturbations**  
J. Duncan et al.  
*arXiv, 2021*  
DOI: [10.48550/ARXIV.2102.11917](https://doi.org/10.48550/ARXIV.2102.11917)  
[https://arxiv.org/abs/2102.11917](https://arxiv.org/abs/2102.11917)

**Ensembles of Networks Produced from Neural Architecture Search**  
E. J. Herron, S. R. Young, and T. E. Potok  
*International Conference on High Performance Computing, 2020*  
Pages 223-234

**Applying Image Feature Extraction to Cluttered Scientific Repositories**  
E. Herron, T. J. Skluzacek, I. Foster, and K. Chard  
*2017*

## Selected Presentations

**SciTrust: Evaluating the Trustworthiness of Large Language Models for Science**  
*2024 Monterey Data Conference, Monterey, California*  
Poster Presentation

**Generalized Differentiable Neural Architecture Search with Performance and Stability Improvements for Scientific Applications**  
*SOS26 2024, Cocoa Beach, FL*  
Poster Presentation

**ICDARTS: Improving the Stability of Cyclic DARTS**  
*2022 21st IEEE International Conference on Machine Learning and Applications*  
Nassau, The Bahamas

**Ensembles of Neural Networks Produced from Neural Architecture Search**  
*Women in High Performance Computing Workshop, SuperComputing 2020*  
Virtual Presentation

**Ensembles of Neural Networks Produced from Neural Architecture Search**  
*The International Conference on High Performance Computing 2020*  
Virtual Presentation

**Applying Image Feature Extraction to Cluttered Scientific Repositories**  
*Student Research Competition Poster Session, SuperComputing 2017*  
Denver, CO
## Education

**Ph.D. in Data Science & Engineering**  
University of Tennessee, Knoxville  
2018 - 2023
- Thesis: Generalized Differentiable Neural Architecture Search with Scaling and Stability Improvements
- Advisor: Dr. Steven R. Young
- GPA: 3.95 / 4.00

**B.S. in Computational Science**  
Mercer University  
2014 - 2018
- Graduated Summa Cum Laude
- GPA: 3.94 / 4.00

## Experience

**Oak Ridge National Laboratory**  
*Postdoctoral Research Associate* (Jan 2024 - Present)
- Developing pipeline for assessing the trustworthiness of large language models for science
- Incorporating evolutionary neural architecture search into mixture of experts (MoE) transformer architectures
- Researching LLM-based hypothesis generation in NLP and materials science

*Graduate Research Assistant* (Aug 2018 - Dec 2023)
- Developed stability improvements to CDARTS NAS algorithm
- Researched and implemented selection algorithms for ORNL's MENNDL NAS software
- Collaborated on creation of NIEHS document mining pipeline

## Professional Service

- ORNL Traveling Science Fair Volunteer (2024)
- ORNL Undergraduate Summer Internship Program Mentor (2024)
- Assistant Instructor, ORNLAI Summer Institute Tutorial (2024)
- ICML 2022 Reviewer - Top 10%
- Bredesen Center Peer Mentor (2022-2023)

## Awards & Honors

- OLCF Director's Discretion Project - 20,000 Summit Hours (2022-2023)
- Bredesen Center Data Science & Engineering Fellowship (2018)
- ACM Student Poster Competition Undergraduate Semifinalist (2017)
- Outstanding Student in Computational Science, Mercer University (2016-2018)
- Summa Cum Laude, Mercer University (2018)

## Professional Affiliations

- Institute of Electrical and Electronics Engineers (IEEE) Member (2022 - Present)
- Association for Computing Machinery (ACM) Member (2017 - Present)

